{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Midi Processing\n",
    "from music21 import *\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "# System\n",
    "import os\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midi Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MIDI Functions\n",
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "    \n",
    "    key = midi.analyze('key').tonic\n",
    "    mid = midi.transpose(interval.Interval(key, pitch.Pitch('C')))\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wtc1011.mid', 'wtc1021.mid', 'wtc1031.mid', 'wtc1041.mid', 'wtc1051.mid', 'wtc1061.mid', 'wtc1071.mid', 'wtc1081.mid', 'wtc1091.mid', 'wtc1101.mid', 'wtc1111.mid', 'wtc1121.mid', 'wtc1131.mid', 'wtc1141.mid', 'wtc1151.mid', 'wtc1161.mid', 'wtc1171.mid', 'wtc1181.mid', 'wtc1191.mid', 'wtc1201.mid']\n",
      "\n",
      "Loading Music File: ./data/bach/wtc1011.mid\n",
      "Loading Music File: ./data/bach/wtc1021.mid\n",
      "Loading Music File: ./data/bach/wtc1031.mid\n",
      "Loading Music File: ./data/bach/wtc1041.mid\n",
      "Loading Music File: ./data/bach/wtc1051.mid\n",
      "Loading Music File: ./data/bach/wtc1061.mid\n",
      "Loading Music File: ./data/bach/wtc1071.mid\n",
      "Loading Music File: ./data/bach/wtc1081.mid\n",
      "Loading Music File: ./data/bach/wtc1091.mid\n",
      "Loading Music File: ./data/bach/wtc1101.mid\n",
      "Loading Music File: ./data/bach/wtc1111.mid\n",
      "Loading Music File: ./data/bach/wtc1121.mid\n",
      "Loading Music File: ./data/bach/wtc1131.mid\n",
      "Loading Music File: ./data/bach/wtc1141.mid\n",
      "Loading Music File: ./data/bach/wtc1151.mid\n",
      "Loading Music File: ./data/bach/wtc1161.mid\n",
      "Loading Music File: ./data/bach/wtc1171.mid\n",
      "Loading Music File: ./data/bach/wtc1181.mid\n",
      "Loading Music File: ./data/bach/wtc1191.mid\n",
      "Loading Music File: ./data/bach/wtc1201.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atawz\\miniconda3\\envs\\pyotr_env_v2\\lib\\site-packages\\ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Read-in all files\n",
    "path = './data/bach/'\n",
    "files= [i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "print(files)\n",
    "print()\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midi File Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "# No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([79.,  3.,  4.,  3.,  3.,  6.,  7.,  9.,  4.,  1.]),\n",
       " array([  1. ,  62.8, 124.6, 186.4, 248.2, 310. , 371.8, 433.6, 495.4,\n",
       "        557.2, 619. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 313
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "# Note Frequency filters - currently not filtering\n",
    "frequent_notes = [note_ for note_, count in freq.items() if count>=0]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atawz\\miniconda3\\envs\\pyotr_env_v2\\lib\\site-packages\\ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['C4' 'E4' 'G4' ... 'A4' 'D5' 'F5']\n",
      " ['E4' 'G4' 'C5' ... 'D5' 'F5' 'B3']\n",
      " ['G4' 'C5' 'E5' ... 'F5' 'B3' 'D4']\n",
      " ...\n",
      " ['C#4' 'B-4' 'E3' ... 'A5' 'D4' 'G#5']\n",
      " ['B-4' 'E3' 'C#4' ... 'D4' 'G#5' '9.1.4']\n",
      " ['E3' 'C#4' 'F4' ... 'G#5' '9.1.4' 'E4']]\n",
      "['B3' 'D4' 'G4' ... '9.1.4' 'E4' 'A3']\n"
     ]
    }
   ],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# Encoding as no_of_timesteps X with the no_of_timesteps's note as output, N:1\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22  57  90 ...  61  14  68]\n",
      " [ 57  90   9 ...  14  68  43]\n",
      " [ 90   9 105 ...  68  43  69]\n",
      " ...\n",
      " [ 25  78  91 ...  77  69  54]\n",
      " [ 78  91  25 ...  69  54  30]\n",
      " [ 91  25 103 ...  54  30  57]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding Dictionary, X\n",
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)\n",
    "print(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43 69 90 ... 31 57  3]\n"
     ]
    }
   ],
   "source": [
    "# Encoding Dictionary, y\n",
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq = np.array([y_note_to_int[i] for i in y])\n",
    "print(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models - Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def lstm():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "    model.add(LSTM(128,return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(len(unique_notes)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def wave_net_sim():\n",
    "    model = Sequential()\n",
    "    \n",
    "    #embedding layer\n",
    "    model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "    model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPool1D(2))\n",
    "\n",
    "    model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPool1D(2))\n",
    "\n",
    "    model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPool1D(2))\n",
    "\n",
    "    #model.add(Conv1D(256,5,activation='relu'))    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(len(unique_y), activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 3.9135\n",
      "Epoch 00001: val_loss improved from inf to 3.80314, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 3.9134 - val_loss: 3.8031\n",
      "Epoch 2/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 3.7880\n",
      "Epoch 00002: val_loss improved from 3.80314 to 3.79439, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 58ms/step - loss: 3.7880 - val_loss: 3.7944\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.7760\n",
      "Epoch 00003: val_loss improved from 3.79439 to 3.75497, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 3.7760 - val_loss: 3.7550\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.6868\n",
      "Epoch 00004: val_loss improved from 3.75497 to 3.66923, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 3.6868 - val_loss: 3.6692\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.6240\n",
      "Epoch 00005: val_loss improved from 3.66923 to 3.58410, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 3.6240 - val_loss: 3.5841\n",
      "Epoch 6/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 3.5109\n",
      "Epoch 00006: val_loss improved from 3.58410 to 3.48199, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 51ms/step - loss: 3.5111 - val_loss: 3.4820\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.4002- ETA: 1s\n",
      "Epoch 00007: val_loss improved from 3.48199 to 3.39269, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 51ms/step - loss: 3.4002 - val_loss: 3.3927\n",
      "Epoch 8/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 3.3188\n",
      "Epoch 00008: val_loss improved from 3.39269 to 3.34122, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 3.3192 - val_loss: 3.3412\n",
      "Epoch 9/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 3.2521\n",
      "Epoch 00009: val_loss improved from 3.34122 to 3.30583, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 3.2527 - val_loss: 3.3058\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.1904\n",
      "Epoch 00010: val_loss improved from 3.30583 to 3.27169, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 3.1904 - val_loss: 3.2717\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.1296\n",
      "Epoch 00011: val_loss improved from 3.27169 to 3.23826, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 3.1296 - val_loss: 3.2383\n",
      "Epoch 12/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 3.0683\n",
      "Epoch 00012: val_loss improved from 3.23826 to 3.21772, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 3.0691 - val_loss: 3.2177\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.9988\n",
      "Epoch 00013: val_loss improved from 3.21772 to 3.20470, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 2.9988 - val_loss: 3.2047\n",
      "Epoch 14/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.9309- ETA: 0s - loss: 2.930\n",
      "Epoch 00014: val_loss improved from 3.20470 to 3.17221, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 56ms/step - loss: 2.9303 - val_loss: 3.1722\n",
      "Epoch 15/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.8535- ETA: \n",
      "Epoch 00015: val_loss improved from 3.17221 to 3.15459, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 2.8533 - val_loss: 3.1546\n",
      "Epoch 16/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.7729- ETA: 3s - loss: 2.758 - ETA: 3s - loss: 2.772  - ETA: 1s\n",
      "Epoch 00016: val_loss improved from 3.15459 to 3.14193, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 2.7726 - val_loss: 3.1419\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.6899- ETA: 0s - loss: 2.\n",
      "Epoch 00017: val_loss did not improve from 3.14193\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 2.6899 - val_loss: 3.1566\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.6025- ETA - ETA: - ETA: 1s - loss: 2.5 - ETA: 0s - lo\n",
      "Epoch 00018: val_loss improved from 3.14193 to 3.13580, saving model to /models\\best_model_lstm_1.h5\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 2.6025 - val_loss: 3.1358\n",
      "Epoch 19/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.5103- ETA: 3s - l - ETA: 2s - los - ET - ETA: 0s - loss: 2.5089\n",
      "Epoch 00019: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 2.5094 - val_loss: 3.1520\n",
      "Epoch 20/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.3982\n",
      "Epoch 00020: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 2.3986 - val_loss: 3.1630\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.3058\n",
      "Epoch 00021: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 51ms/step - loss: 2.3058 - val_loss: 3.1878\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.1956\n",
      "Epoch 00022: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 51ms/step - loss: 2.1956 - val_loss: 3.1933\n",
      "Epoch 23/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.0882\n",
      "Epoch 00023: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 2.0871 - val_loss: 3.2630\n",
      "Epoch 24/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.9833\n",
      "Epoch 00024: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.9828 - val_loss: 3.3153\n",
      "Epoch 25/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.8622\n",
      "Epoch 00025: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 57ms/step - loss: 1.8634 - val_loss: 3.3664\n",
      "Epoch 26/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7552\n",
      "Epoch 00026: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.7565 - val_loss: 3.4502\n",
      "Epoch 27/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6471\n",
      "Epoch 00027: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 57ms/step - loss: 1.6473 - val_loss: 3.4920\n",
      "Epoch 28/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5302- ETA: 0s - loss: \n",
      "Epoch 00028: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 57ms/step - loss: 1.5310 - val_loss: 3.5581\n",
      "Epoch 29/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.3974\n",
      "Epoch 00029: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 59ms/step - loss: 1.3973 - val_loss: 3.6301\n",
      "Epoch 30/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.2879\n",
      "Epoch 00030: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 58ms/step - loss: 1.2894 - val_loss: 3.7791\n",
      "Epoch 31/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.1760- ETA:  - ETA: 1s - loss: - ETA: 1s - loss: 1 - ETA: 0s - loss:\n",
      "Epoch 00031: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.1751 - val_loss: 3.8881\n",
      "Epoch 32/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.0751\n",
      "Epoch 00032: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.0742 - val_loss: 3.9559\n",
      "Epoch 33/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.9658\n",
      "Epoch 00033: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 0.9664 - val_loss: 4.0896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.8665\n",
      "Epoch 00034: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 57ms/step - loss: 0.8674 - val_loss: 4.2168\n",
      "Epoch 35/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.7602\n",
      "Epoch 00035: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 60ms/step - loss: 0.7602 - val_loss: 4.3492\n",
      "Epoch 36/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6703- ETA: 1\n",
      "Epoch 00036: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 0.6707 - val_loss: 4.5041\n",
      "Epoch 37/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.5910\n",
      "Epoch 00037: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 57ms/step - loss: 0.5911 - val_loss: 4.6541\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5134\n",
      "Epoch 00038: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 0.5134 - val_loss: 4.7779\n",
      "Epoch 39/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.4446\n",
      "Epoch 00039: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 0.4447 - val_loss: 4.9429\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3813- ETA: - ETA: 0s - loss: \n",
      "Epoch 00040: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 0.3813 - val_loss: 5.0671\n",
      "Epoch 41/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.3314- ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0\n",
      "Epoch 00041: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 0.3314 - val_loss: 5.2280\n",
      "Epoch 42/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.2611- ETA: 2s - loss:  - ETA: 2s - loss: 0\n",
      "Epoch 00042: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 0.2613 - val_loss: 5.4001\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.2197\n",
      "Epoch 00043: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 0.2197 - val_loss: 5.5603\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1799\n",
      "Epoch 00044: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 51ms/step - loss: 0.1799 - val_loss: 5.7102\n",
      "Epoch 45/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1491- ETA: 2s - l - ETA: 1s - loss: - ETA: 0s - loss\n",
      "Epoch 00045: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 0.1493 - val_loss: 5.8219\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1196- E - ETA: 0s - loss:  - ETA: 0s - loss: 0.119\n",
      "Epoch 00046: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 0.1196 - val_loss: 5.9871\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0976- ETA: 2s -  - ETA: 1\n",
      "Epoch 00047: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 0.0976 - val_loss: 6.1123\n",
      "Epoch 48/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0798- ETA: 3s - loss: 0. - ETA:  - ETA: 1s\n",
      "Epoch 00048: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 0.0802 - val_loss: 6.2693\n",
      "Epoch 49/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0787\n",
      "Epoch 00049: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 57ms/step - loss: 0.0787 - val_loss: 6.3735\n",
      "Epoch 50/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0659\n",
      "Epoch 00050: val_loss did not improve from 3.13580\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 0.0659 - val_loss: 6.5110\n"
     ]
    }
   ],
   "source": [
    "model = lstm()\n",
    "mc = ModelCheckpoint('./models/best_model_lstm_1.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
    "history = model.fit(np.array(x_tr),\n",
    "                    np.array(y_tr),\n",
    "                    batch_size=128,\n",
    "                    epochs=30, \n",
    "                    validation_data=(np.array(x_val),\n",
    "                                     np.array(y_val)),\n",
    "                    verbose=1, \n",
    "                    callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.8923\n",
      "Epoch 00001: val_loss improved from inf to 3.75513, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 3.8923 - val_loss: 3.7551\n",
      "Epoch 2/50\n",
      "86/88 [============================>.] - ETA: 0s - loss: 3.6101- ET\n",
      "Epoch 00002: val_loss improved from 3.75513 to 3.59541, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 17ms/step - loss: 3.6085 - val_loss: 3.5954\n",
      "Epoch 3/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 3.4789\n",
      "Epoch 00003: val_loss improved from 3.59541 to 3.50797, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 17ms/step - loss: 3.4763 - val_loss: 3.5080\n",
      "Epoch 4/50\n",
      "86/88 [============================>.] - ETA: 0s - loss: 3.3846\n",
      "Epoch 00004: val_loss improved from 3.50797 to 3.45920, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 3.3831 - val_loss: 3.4592\n",
      "Epoch 5/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 3.3010\n",
      "Epoch 00005: val_loss improved from 3.45920 to 3.38857, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 3.3014 - val_loss: 3.3886\n",
      "Epoch 6/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 3.2437\n",
      "Epoch 00006: val_loss improved from 3.38857 to 3.33131, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 3.2430 - val_loss: 3.3313\n",
      "Epoch 7/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 3.1897\n",
      "Epoch 00007: val_loss improved from 3.33131 to 3.32699, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 3.1905 - val_loss: 3.3270\n",
      "Epoch 8/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 3.1436\n",
      "Epoch 00008: val_loss improved from 3.32699 to 3.30152, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 3.1456 - val_loss: 3.3015\n",
      "Epoch 9/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 3.0898\n",
      "Epoch 00009: val_loss improved from 3.30152 to 3.23364, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 3.0890 - val_loss: 3.2336\n",
      "Epoch 10/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 3.0434\n",
      "Epoch 00010: val_loss did not improve from 3.23364\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 3.0447 - val_loss: 3.2559\n",
      "Epoch 11/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 3.0310\n",
      "Epoch 00011: val_loss improved from 3.23364 to 3.21282, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 3.0306 - val_loss: 3.2128\n",
      "Epoch 12/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.9803\n",
      "Epoch 00012: val_loss did not improve from 3.21282\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.9826 - val_loss: 3.2187\n",
      "Epoch 13/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.9412\n",
      "Epoch 00013: val_loss improved from 3.21282 to 3.18733, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.9436 - val_loss: 3.1873\n",
      "Epoch 14/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.9225\n",
      "Epoch 00014: val_loss improved from 3.18733 to 3.18369, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.9209 - val_loss: 3.1837\n",
      "Epoch 15/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.8900\n",
      "Epoch 00015: val_loss improved from 3.18369 to 3.15554, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.8879 - val_loss: 3.1555\n",
      "Epoch 16/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.8416\n",
      "Epoch 00016: val_loss did not improve from 3.15554\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.8407 - val_loss: 3.1648\n",
      "Epoch 17/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.8176\n",
      "Epoch 00017: val_loss did not improve from 3.15554\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.8196 - val_loss: 3.1607\n",
      "Epoch 18/50\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.7777\n",
      "Epoch 00018: val_loss improved from 3.15554 to 3.14087, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.7801 - val_loss: 3.1409\n",
      "Epoch 19/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.7507\n",
      "Epoch 00019: val_loss did not improve from 3.14087\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.7521 - val_loss: 3.1459\n",
      "Epoch 20/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.7154\n",
      "Epoch 00020: val_loss did not improve from 3.14087\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.7207 - val_loss: 3.1421\n",
      "Epoch 21/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.7019\n",
      "Epoch 00021: val_loss improved from 3.14087 to 3.12941, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.7008 - val_loss: 3.1294\n",
      "Epoch 22/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.6671\n",
      "Epoch 00022: val_loss improved from 3.12941 to 3.12302, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.6687 - val_loss: 3.1230\n",
      "Epoch 23/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.6412\n",
      "Epoch 00023: val_loss did not improve from 3.12302\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.6415 - val_loss: 3.1298\n",
      "Epoch 24/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.6202\n",
      "Epoch 00024: val_loss improved from 3.12302 to 3.12212, saving model to /models\\best_model_wave_net_1.h5\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.6240 - val_loss: 3.1221\n",
      "Epoch 25/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.5820\n",
      "Epoch 00025: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.5829 - val_loss: 3.1372\n",
      "Epoch 26/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.5648\n",
      "Epoch 00026: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.5632 - val_loss: 3.1322\n",
      "Epoch 27/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.5323\n",
      "Epoch 00027: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.5353 - val_loss: 3.1429\n",
      "Epoch 28/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.5048\n",
      "Epoch 00028: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.5085 - val_loss: 3.1323\n",
      "Epoch 29/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.4726\n",
      "Epoch 00029: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.4727 - val_loss: 3.1409\n",
      "Epoch 30/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.4641\n",
      "Epoch 00030: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.4655 - val_loss: 3.1487\n",
      "Epoch 31/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.4237\n",
      "Epoch 00031: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.4253 - val_loss: 3.1521\n",
      "Epoch 32/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.4014\n",
      "Epoch 00032: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.4058 - val_loss: 3.1569\n",
      "Epoch 33/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.3897\n",
      "Epoch 00033: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.3887 - val_loss: 3.1623\n",
      "Epoch 34/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.3556\n",
      "Epoch 00034: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.3584 - val_loss: 3.1605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.3414\n",
      "Epoch 00035: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 2.3399 - val_loss: 3.1635\n",
      "Epoch 36/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.3262\n",
      "Epoch 00036: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.3286 - val_loss: 3.1700\n",
      "Epoch 37/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.2936\n",
      "Epoch 00037: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.2960 - val_loss: 3.1760\n",
      "Epoch 38/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.2843\n",
      "Epoch 00038: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.2854 - val_loss: 3.1939\n",
      "Epoch 39/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.2641\n",
      "Epoch 00039: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.2667 - val_loss: 3.1726\n",
      "Epoch 40/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.2357\n",
      "Epoch 00040: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.2418 - val_loss: 3.1900\n",
      "Epoch 41/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.2320\n",
      "Epoch 00041: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.2339 - val_loss: 3.2207\n",
      "Epoch 42/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.2142\n",
      "Epoch 00042: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.2166 - val_loss: 3.1993\n",
      "Epoch 43/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.1745\n",
      "Epoch 00043: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.1738 - val_loss: 3.2245\n",
      "Epoch 44/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.1581\n",
      "Epoch 00044: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.1571 - val_loss: 3.2332\n",
      "Epoch 45/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.1370\n",
      "Epoch 00045: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.1375 - val_loss: 3.2432\n",
      "Epoch 46/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.1359\n",
      "Epoch 00046: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.1392 - val_loss: 3.2362\n",
      "Epoch 47/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.1150\n",
      "Epoch 00047: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.1178 - val_loss: 3.2575\n",
      "Epoch 48/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.0949\n",
      "Epoch 00048: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.0958 - val_loss: 3.2580\n",
      "Epoch 49/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.0836\n",
      "Epoch 00049: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.0869 - val_loss: 3.2640\n",
      "Epoch 50/50\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.0680\n",
      "Epoch 00050: val_loss did not improve from 3.12212\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 2.0709 - val_loss: 3.2811\n"
     ]
    }
   ],
   "source": [
    "model = wave_net_sim()\n",
    "mc = ModelCheckpoint('./models/best_model_wave_net_1.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
    "history = model.fit(np.array(x_tr),\n",
    "                    np.array(y_tr),\n",
    "                    batch_size=128,\n",
    "                    epochs=30, \n",
    "                    validation_data=(np.array(x_val),\n",
    "                                     np.array(y_val)),\n",
    "                    verbose=1, \n",
    "                    callbacks=[mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: /models/best_model_lstm_1.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-fd1590c1e259>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/models/best_model_lstm_1.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\pyotr_env_v2\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m       \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\pyotr_env_v2\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    111\u001b[0m                   (export_dir,\n\u001b[0;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: /models/best_model_lstm_1.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "model = load_model('/models/best_model_lstm_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 61, 42, 42, 50, 61, 50, 50, 50, 90, 90, 90, 90, 90, 90, 90, 90, 61, 90, 90, 42, 90, 90, 90, 42, 61, 61, 112, 61, 61, 78]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with random note input\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "random_music = x_val[ind]\n",
    "predictions=[]\n",
    "for i in range(32):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 57, 90, 9, 105, 90, 9, 105, 22, 57, 90, 9, 105, 90, 9, 105, 22, 69, 61, 14, 68, 61, 14, 68, 22, 69, 61, 14, 68, 61, 14, 68, 43, 69, 90, 14, 68, 90, 14, 14, 43, 69, 90, 9, 68, 90, 9, 9, 9, 57, 9, 9, 9, 90, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with seeded input\n",
    "random_music = x_seq[0]\n",
    "predictions = list(random_music)\n",
    "for i in range(32):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='./data/generated/wavenet_test.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyotr 3.7 - Tensorflow/Keras",
   "language": "python",
   "name": "pyotr_env_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
