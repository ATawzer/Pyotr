{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external imports\n",
    "import music21 as m21\n",
    "import ast\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.train' from 'D:\\\\Documents\\\\GitHub\\\\Pyotr\\\\src\\\\train.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload cell\n",
    "from src.midi import *\n",
    "from src.train import *\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['src.midi'])\n",
    "importlib.reload(sys.modules['src.train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and Read-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all of the midi files\n",
    "path = './data/sample'\n",
    "mdl = gen_md_from_path(path, by_measure=False, verbose=False)\n",
    "mdm = gen_md_from_path(path, by_measure=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode them (as entire piece)\n",
    "me = MidiEncoder()\n",
    "mdl_enc = {}\n",
    "for piece in mdl:\n",
    "    mdl_enc[piece] = me.Encode(mdl[piece].flat, 'pitch_position_duration_strings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode them (by measure)\n",
    "me = MidiEncoder()\n",
    "mdm_enc = {}\n",
    "for piece in mdm:\n",
    "    mdm_enc[piece] = {}\n",
    "    for i, m in enumerate(mdm[piece]):\n",
    "        mdm_enc[piece][i] = me.Encode(m, 'pitch_position_duration_strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TrainingSet(mdm_enc, by_measure=True, num_notes=1, build_type='next_note')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample network, linear model\n",
    "class Net_1(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, vocab_size):\n",
    "        super(Net_1, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(self.seq_len, 1)\n",
    "\n",
    "    def forward(self, note_sequence, state=None):\n",
    "        x = self.out(note_sequence)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1811]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.7101], requires_grad=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net_1(1, data.get_vocab_size())\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params and setup\n",
    "sequence_length = 10\n",
    "\n",
    "data = TrainingSet(mdm_enc, by_measure=True, num_notes=sequence_length, build_type='next_note')\n",
    "model = Net_1(sequence_length, data.get_vocab_size())\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Running Loss: 724216.0625                  \n",
      "Epoch: 2 | Running Loss: 724078.5                     \n",
      "Epoch: 3 | Running Loss: 723911.6875                 \n",
      "Epoch: 4 | Running Loss: 724488.1875                 \n",
      "Epoch: 5 | Running Loss: 724097.0                    \n",
      "Epoch: 6 | Running Loss: 724488.1875                 \n",
      "Epoch: 7 | Running Loss: 724097.0                    \n",
      "Epoch: 8 | Running Loss: 724488.1875                 \n",
      "Epoch: 9 | Running Loss: 724097.0                    \n",
      "Running Loss: 104276.0390625                         \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atawz\\miniconda3\\envs\\pyotr_env_v1\\lib\\site-packages\\torch\\nn\\modules\\loss.py:93: UserWarning: Using a target size (torch.Size([1, 122])) that is different to the input size (torch.Size([122, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\atawz\\miniconda3\\envs\\pyotr_env_v1\\lib\\site-packages\\torch\\nn\\modules\\loss.py:93: UserWarning: Using a target size (torch.Size([1, 121])) that is different to the input size (torch.Size([121, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Running Loss: 724488.1875            \n",
      "Epoch: 11 | Running Loss: 724097.0                   \n",
      "Epoch: 12 | Running Loss: 724488.1875                \n",
      "Epoch: 13 | Running Loss: 724097.0                   \n",
      "Epoch: 14 | Running Loss: 724488.1875                \n",
      "Epoch: 15 | Running Loss: 724097.0                   \n",
      "Epoch: 16 | Running Loss: 724488.1875                \n",
      "Epoch: 17 | Running Loss: 724097.0                   \n",
      "Epoch: 18 | Running Loss: 724488.1875                \n",
      "Epoch: 19 | Running Loss: 724097.0                   \n",
      "Epoch: 20 | Running Loss: 724488.1875                \n",
      "Epoch: 21 | Running Loss: 724097.0                   \n",
      "Epoch: 22 | Running Loss: 724488.1875                \n",
      "Epoch: 23 | Running Loss: 724097.0                   \n",
      "Epoch: 24 | Running Loss: 724488.1875                \n",
      "Epoch: 25 | Running Loss: 724097.0                   \n",
      "Epoch: 26 | Running Loss: 724488.1875                \n",
      "Epoch: 27 | Running Loss: 724097.0                   \n",
      "Epoch: 28 | Running Loss: 724488.1875                \n",
      "Epoch: 29 | Running Loss: 724097.0                   \n",
      "Epoch: 30 | Running Loss: 724488.1875                \n",
      "Epoch: 31 | Running Loss: 724097.0                   \n",
      "Epoch: 32 | Running Loss: 724488.1875                \n",
      "Epoch: 33 | Running Loss: 724097.0                   \n",
      "Epoch: 34 | Running Loss: 724488.1875                \n",
      "Epoch: 35 | Running Loss: 724097.0                   \n",
      "Epoch: 36 | Running Loss: 724488.1875                \n",
      "Epoch: 37 | Running Loss: 724097.0                   \n",
      "Epoch: 38 | Running Loss: 724488.1875                \n",
      "Epoch: 39 | Running Loss: 724097.0                   \n",
      "Epoch: 40 | Running Loss: 724488.1875                \n",
      "Epoch: 41 | Running Loss: 724097.0                   \n",
      "Epoch: 42 | Running Loss: 724488.1875                \n",
      "Epoch: 43 | Running Loss: 724097.0                   \n",
      "Epoch: 44 | Running Loss: 724488.1875                \n",
      "Epoch: 45 | Running Loss: 724097.0                   \n",
      "Epoch: 46 | Running Loss: 724488.1875                \n",
      "Epoch: 47 | Running Loss: 724097.0                   \n",
      "Epoch: 48 | Running Loss: 724488.1875                \n",
      "Epoch: 49 | Running Loss: 724097.0                   \n",
      "Epoch: 50 | Running Loss: 724488.1875                \n",
      "Epoch: 51 | Running Loss: 724097.0                   \n",
      "Epoch: 52 | Running Loss: 724488.1875                \n",
      "Epoch: 53 | Running Loss: 724097.0                   \n",
      "Epoch: 54 | Running Loss: 724488.1875                \n",
      "Epoch: 55 | Running Loss: 724097.0                   \n",
      "Epoch: 56 | Running Loss: 724488.1875                \n",
      "Epoch: 57 | Running Loss: 724097.0                   \n",
      "Epoch: 58 | Running Loss: 724488.1875                \n",
      "Epoch: 59 | Running Loss: 724097.0                   \n",
      "Epoch: 60 | Running Loss: 724488.1875                \n",
      "Epoch: 61 | Running Loss: 724097.0                   \n",
      "Epoch: 62 | Running Loss: 724488.1875                \n",
      "Epoch: 63 | Running Loss: 724097.0                   \n",
      "Epoch: 64 | Running Loss: 724488.1875                \n",
      "Epoch: 65 | Running Loss: 724097.0                   \n",
      "Epoch: 66 | Running Loss: 724488.1875                \n",
      "Epoch: 67 | Running Loss: 724097.0                   \n",
      "Epoch: 68 | Running Loss: 724488.1875                \n",
      "Epoch: 69 | Running Loss: 724097.0                   \n",
      "Epoch: 70 | Running Loss: 724488.1875                \n",
      "Epoch: 71 | Running Loss: 724097.0                   \n",
      "Epoch: 72 | Running Loss: 724488.1875                \n",
      "Epoch: 73 | Running Loss: 724097.0                   \n",
      "Epoch: 74 | Running Loss: 724488.1875                \n",
      "Epoch: 75 | Running Loss: 724097.0                   \n",
      "Epoch: 76 | Running Loss: 724488.1875                \n",
      "Epoch: 77 | Running Loss: 724097.0                   \n",
      "Epoch: 78 | Running Loss: 724488.1875                \n",
      "Epoch: 79 | Running Loss: 724097.0                   \n",
      "Epoch: 80 | Running Loss: 724488.1875                \n",
      "Epoch: 81 | Running Loss: 724097.0                   \n",
      "Epoch: 82 | Running Loss: 724488.1875                \n",
      "Epoch: 83 | Running Loss: 724097.0                   \n",
      "Epoch: 84 | Running Loss: 724488.1875                \n",
      "Epoch: 85 | Running Loss: 724097.0                   \n",
      "Epoch: 86 | Running Loss: 724488.1875                \n",
      "Epoch: 87 | Running Loss: 724097.0                   \n",
      "Epoch: 88 | Running Loss: 724488.1875                \n",
      "Epoch: 89 | Running Loss: 724097.0                   \n",
      "Epoch: 90 | Running Loss: 724488.1875                \n",
      "Epoch: 91 | Running Loss: 724097.0                   \n",
      "Epoch: 92 | Running Loss: 724488.1875                \n",
      "Epoch: 93 | Running Loss: 724097.0                   \n",
      "Epoch: 94 | Running Loss: 724488.1875                \n",
      "Epoch: 95 | Running Loss: 724097.0                   \n",
      "Epoch: 96 | Running Loss: 724488.1875                \n",
      "Epoch: 97 | Running Loss: 724097.0                   \n",
      "Epoch: 98 | Running Loss: 724488.1875                \n",
      "Epoch: 99 | Running Loss: 724097.0                   \n",
      "Epoch: 100 | Running Loss: 724488.1875               \n",
      "Epoch: 101 | Running Loss: 724097.0                  \n",
      "Epoch: 102 | Running Loss: 724488.1875               \n",
      "Epoch: 103 | Running Loss: 724097.0                  \n",
      "Epoch: 104 | Running Loss: 724488.1875               \n",
      "Epoch: 105 | Running Loss: 724097.0                  \n",
      "Epoch: 106 | Running Loss: 724488.1875               \n",
      "Epoch: 107 | Running Loss: 724097.0                  \n",
      "Epoch: 108 | Running Loss: 724488.1875               \n",
      "Epoch: 109 | Running Loss: 724097.0                  \n",
      "Epoch: 110 | Running Loss: 724488.1875               \n",
      "Epoch: 111 | Running Loss: 724097.0                  \n",
      "Epoch: 112 | Running Loss: 724488.1875               \n",
      "Epoch: 113 | Running Loss: 724097.0                  \n",
      "Epoch: 114 | Running Loss: 724488.1875               \n",
      "Epoch: 115 | Running Loss: 724097.0                  \n",
      "Epoch: 116 | Running Loss: 724488.1875               \n",
      "Epoch: 117 | Running Loss: 724097.0                  \n",
      "Epoch: 118 | Running Loss: 724488.1875               \n",
      "Epoch: 119 | Running Loss: 724097.0                  \n",
      "Epoch: 120 | Running Loss: 724488.1875               \n",
      "Epoch: 121 | Running Loss: 724097.0                  \n",
      "Epoch: 122 | Running Loss: 724488.1875               \n",
      "Epoch: 123 | Running Loss: 724097.0                  \n",
      "Epoch: 124 | Running Loss: 724488.1875               \n",
      "Epoch: 125 | Running Loss: 724097.0                  \n",
      "Epoch: 126 | Running Loss: 724488.1875               \n",
      "Epoch: 127 | Running Loss: 724097.0                  \n",
      "Epoch: 128 | Running Loss: 724488.1875               \n",
      "Epoch: 129 | Running Loss: 724097.0                  \n",
      "Epoch: 130 | Running Loss: 724488.1875               \n",
      "Epoch: 131 | Running Loss: 724097.0                  \n",
      "Epoch: 132 | Running Loss: 724488.1875               \n",
      "Epoch: 133 | Running Loss: 724097.0                  \n",
      "Epoch: 134 | Running Loss: 724488.1875               \n",
      "Epoch: 135 | Running Loss: 724097.0                  \n",
      "Epoch: 136 | Running Loss: 724488.1875               \n",
      "Epoch: 137 | Running Loss: 724097.0                  \n",
      "Epoch: 138 | Running Loss: 724488.1875               \n",
      "Epoch: 139 | Running Loss: 724097.0                  \n",
      "Epoch: 140 | Running Loss: 724488.1875               \n",
      "Epoch: 141 | Running Loss: 724097.0                  \n",
      "Epoch: 142 | Running Loss: 724488.1875               \n",
      "Epoch: 143 | Running Loss: 724097.0                  \n",
      "Epoch: 144 | Running Loss: 724488.1875               \n",
      "Epoch: 145 | Running Loss: 724097.0                  \n",
      "Epoch: 146 | Running Loss: 724488.1875               \n",
      "Epoch: 147 | Running Loss: 724097.0                  \n",
      "Epoch: 148 | Running Loss: 724488.1875               \n",
      "Epoch: 149 | Running Loss: 724097.0                  \n",
      "Running Loss: 365211.40625                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 196 | Running Loss: 724488.1875         \n",
      "Epoch: 197 | Running Loss: 724097.0                  \n",
      "Epoch: 198 | Running Loss: 724488.1875               \n",
      "Epoch: 199 | Running Loss: 724097.0                  \n",
      "Epoch: 200 | Running Loss: 724488.1875               \n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(200):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for batch in np.array_split([x for x in range(0, len(data.Xnp))], 20):\n",
    "        \n",
    "        # Reset\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Data\n",
    "        inputs = torch.tensor(data.Xnp[batch.tolist()], dtype=torch.float)\n",
    "        target = torch.tensor([np.array(data.ynp)[batch.tolist()]], dtype=torch.float)\n",
    "\n",
    "        # Forward\n",
    "        score = model(inputs)\n",
    "\n",
    "        # Backward\n",
    "        loss = loss_function(score, target)\n",
    "        running_loss += loss\n",
    "        print(f\"Running Loss: {running_loss}                      \", end='\\r', flush=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch: {epoch+1} | Running Loss: {running_loss}        \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_to_vocab(output):\n",
    "    \n",
    "    min_num = min(output)\n",
    "    max_num = max(output)\n",
    "    r = max_num - min_num\n",
    "    \n",
    "    new_out = [math.ceil((data.get_vocab_size()-1)*((x-min_num)/r)) for x in output]\n",
    "    return [data.idx_token_map[x] for x in new_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "start = data.Xnp[0]\n",
    "num_preds = 30\n",
    "pred_input = torch.tensor(start, dtype=torch.float)\n",
    "out_notes = []\n",
    "\n",
    "for i in range(0, num_preds):\n",
    "    next_note = model(pred_input)\n",
    "    out_notes.append(next_note.item())\n",
    "    pred_input = torch.cat((pred_input[1:], next_note/out_notes[i-1]))\n",
    "    \n",
    "adjusted_output = scale_to_vocab(out_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G2:0.0:1.5',\n",
       " 'B5:0.75:0.25',\n",
       " 'F3:0.0:1.0',\n",
       " 'D5:3.25:0.25',\n",
       " 'B2:2.5:0.25',\n",
       " 'G#3:1.5:0.25',\n",
       " 'D3:1.75:0.25',\n",
       " 'C4:1.0:0.25',\n",
       " 'C3:0.25:1.75',\n",
       " 'D3:0.0:2.0',\n",
       " 'E4:2.25:1.75',\n",
       " 'C4:0.0:2.0',\n",
       " 'G4:1.25:0.25',\n",
       " 'E4:2.25:1.75',\n",
       " 'E4:2.25:1.75',\n",
       " 'C5:3.5:0.25',\n",
       " 'G4:3.25:0.25',\n",
       " 'E5:3.75:0.25',\n",
       " 'F5:1.0:0.25',\n",
       " 'F5:1.75:0.25',\n",
       " 'D5:2.75:0.25',\n",
       " 'D5:3.5:0.25',\n",
       " 'B3:0.0:2.0',\n",
       " 'D5:3.5:0.25',\n",
       " 'D5:3.5:0.25',\n",
       " 'D5:3.5:0.25',\n",
       " 'A4:3.25:0.25',\n",
       " 'A4:3.25:0.25',\n",
       " 'A4:3.25:0.25',\n",
       " 'A4:3.25:0.25']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<music21.stream.Stream 0x24965c08f08>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = MidiWriter()\n",
    "md.Write(adjusted_output, 'pitch_position_duration_strings', 'test3.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyotr_env_v1",
   "language": "python",
   "name": "pyotr_env_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
